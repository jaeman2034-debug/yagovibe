import { useEffect, useState } from "react";
import { addDoc, collection, onSnapshot } from "firebase/firestore";
import { db } from "../../lib/firebase";

export default function VoiceAssistant_AI() {
  const [text, setText] = useState("");
  const [aiResult, setAiResult] = useState<any>(null);
  const [listening, setListening] = useState(false);

  // âœ… ë¸Œë¼ìš°ì € ìŒì„± ì¸ì‹ ê°ì²´
  const SpeechRecognition = typeof window !== "undefined"
    ? (window as any).SpeechRecognition || (window as any).webkitSpeechRecognition
    : null;

  // âœ… Firestore ì‹¤ì‹œê°„ ë¶„ì„ ê²°ê³¼ êµ¬ë…
  useEffect(() => {
    const unsub = onSnapshot(collection(db, "voice_analysis"), (snapshot) => {
      snapshot.docChanges().forEach((change) => {
        if (change.type === "added") {
          const data = change.doc.data();
          console.log("ğŸ§  AI ì‘ë‹µ ìˆ˜ì‹ :", data);
          setAiResult(data);
          speakText(data.aiResult?.summary || "AI ë¶„ì„ ê²°ê³¼ë¥¼ ë¶ˆëŸ¬ì™”ì–´ìš”.");
        }
      });
    });
    return () => unsub();
  }, []);

  // âœ… ìŒì„± ì¸ì‹ ì‹œì‘
  const startListening = () => {
    if (!SpeechRecognition) {
      alert("ìŒì„± ì¸ì‹ì„ ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¸Œë¼ìš°ì €ì…ë‹ˆë‹¤ ğŸ˜¢");
      return;
    }
    const recognition = new SpeechRecognition();
    recognition.lang = "ko-KR";
    recognition.continuous = false;
    recognition.interimResults = false;

    setListening(true);

    recognition.onresult = async (event: any) => {
      const transcript = event.results[0][0].transcript;
      setText(transcript);
      console.log("ğŸ¤ ìŒì„± ì¸ì‹ ê²°ê³¼:", transcript);
      await sendVoiceCommand(transcript, "user_demo");
      setListening(false);
    };

    recognition.onerror = (e: any) => {
      console.error("âŒ ìŒì„± ì¸ì‹ ì˜¤ë¥˜:", e);
      setListening(false);
    };

    recognition.start();
  };

  // âœ… Firestoreë¡œ ìŒì„± ëª…ë ¹ ì „ì†¡
  const sendVoiceCommand = async (text: string, userId: string) => {
    const ref = await addDoc(collection(db, "voice_commands"), {
      text,
      userId,
      createdAt: new Date(),
    });
    console.log("âœ… Firestore ì „ì†¡ ì™„ë£Œ:", ref.id);
  };

  // âœ… TTSë¡œ ìŒì„± ì¬ìƒ
  const speakText = (text: string) => {
    const utter = new SpeechSynthesisUtterance(text);
    utter.lang = "ko-KR";
    utter.rate = 1;
    window.speechSynthesis.speak(utter);
  };

  return (
    <div className="min-h-screen flex flex-col items-center justify-center bg-gray-900 text-white p-6">
      <h1 className="text-2xl font-bold mb-6">ğŸ™ï¸ YAGO VIBE Voice Assistant</h1>

      <button
        onClick={startListening}
        className={`px-6 py-3 rounded-2xl font-semibold shadow-md transition ${listening ? "bg-red-500 animate-pulse" : "bg-blue-600 hover:bg-blue-700"
          }`}
      >
        {listening ? "ğŸ§ ë“£ëŠ” ì¤‘..." : "ğŸ¤ ë§í•˜ê¸° ì‹œì‘"}
      </button>

      {text && (
        <div className="mt-6 p-4 bg-gray-800 rounded-xl w-full max-w-md">
          <p className="text-gray-300 text-sm mb-2">ìŒì„± ì¸ì‹ ê²°ê³¼</p>
          <p className="text-lg">{text}</p>
        </div>
      )}

      {aiResult && (
        <div className="mt-6 p-6 bg-green-800 rounded-2xl shadow-lg w-full max-w-md transition-all">
          <h2 className="text-xl font-semibold mb-2">ğŸ§  AI ë¶„ì„ ê²°ê³¼</h2>
          <p className="text-gray-100">{aiResult.aiResult?.summary || "ë¶„ì„ ì¤‘..."}</p>
          <p className="text-sm text-gray-400 mt-3">
            â± {new Date(aiResult.createdAt?.seconds * 1000).toLocaleString()}
          </p>
        </div>
      )}
    </div>
  );
}
