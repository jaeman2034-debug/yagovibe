import { useState, useRef, useEffect } from 'react';
import { Button } from '@/components/ui/button';
import { Card, CardContent } from '@/components/ui/card';
import { Badge } from '@/components/ui/badge';
import { Mic, Hand, Volume2, MapPin, Calendar } from 'lucide-react';
import { startSTT, synthTTS, detectGesture, synthTTSMultilingual, VADDetector } from '@/lib/voiceux/core';

/**
 * Step 71: Voice UX 2.0 Assistant Panel
 * Multi-Modal AI Extensions & Voice UX 2.0
 */
export default function AssistantPanel() {
    const [text, setText] = useState('');
    const [reply, setReply] = useState('');
    const [listening, setListening] = useState(false);
    const [gesture, setGesture] = useState<string | null>(null);
    const [context, setContext] = useState<any>(null);
    const videoRef = useRef<HTMLVideoElement>(null);
    const streamRef = useRef<MediaStream | null>(null);
    const vadRef = useRef<VADDetector | null>(null);

    useEffect(() => {
        // ì¹´ë©”ë¼ ì´ˆê¸°í™”
        if (videoRef.current) {
            navigator.mediaDevices
                .getUserMedia({ video: true, audio: false })
                .then((stream) => {
                    if (videoRef.current) {
                        videoRef.current.srcObject = stream;
                        streamRef.current = stream;
                    }
                })
                .catch((error) => {
                    console.warn('ì¹´ë©”ë¼ ì ‘ê·¼ ì‹¤íŒ¨:', error);
                });
        }

        return () => {
            if (streamRef.current) {
                streamRef.current.getTracks().forEach((track) => track.stop());
            }
            if (vadRef.current) {
                vadRef.current.stop();
            }
        };
    }, []);

    // ì œìŠ¤ì²˜ ê°ì§€ ë£¨í”„
    useEffect(() => {
        if (!videoRef.current || listening) return;

        const detectLoop = async () => {
            const detectedGesture = await detectGesture(videoRef.current);
            if (detectedGesture && detectedGesture !== gesture) {
                setGesture(detectedGesture);
            }
        };

        const interval = setInterval(detectLoop, 500); // 0.5ì´ˆë§ˆë‹¤ ì²´í¬
        return () => clearInterval(interval);
    }, [listening, gesture]);

    async function handleVoice() {
        try {
            setListening(true);
            setText('');
            setReply('');
            setGesture(null);

            // ìŒì„± ì…ë ¥
            const spoken = await startSTT();
            setText(spoken);

            // ì œìŠ¤ì²˜ ê°ì§€
            const detectedGesture = await detectGesture(videoRef.current);
            if (detectedGesture) {
                setGesture(detectedGesture);
            }

            // NLU ì²˜ë¦¬ (Step 52/58 ì—°ë™)
            // Firebase Functions NLU ì—”ë“œí¬ì¸íŠ¸ ì‚¬ìš©
            const nluEndpoint = import.meta.env.VITE_NLU_ENDPOINT ||
                "https://asia-northeast3-yago-vibe-spt.cloudfunctions.net/nluHandler";

            const contextData = {
                gesture: detectedGesture,
                location: await getCurrentLocation(),
                timestamp: new Date().toISOString(),
            };

            // NLU Handler ë˜ëŠ” GraphCopilot ì—”ì§„ í˜¸ì¶œ
            const response = await fetch(nluEndpoint, {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({
                    text: spoken,
                    context: contextData,
                }),
            }).catch(async () => {
                // Fallback: graphCopilot ì‹œë„
                return await fetch(`${functionsOrigin}/graphCopilot`, {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({
                        query: spoken,
                        context: contextData,
                    }),
                });
            });

            if (!response || !response.ok) {
                throw new Error('NLU ì²˜ë¦¬ ì‹¤íŒ¨');
            }

            const data = await response.json();
            
            // ì‘ë‹µ í˜•ì‹ ì •ê·œí™” (nluHandler vs graphCopilot)
            const replyText = data.reply || data.summary || data.result || data.action || 'ì‘ë‹µì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.';
            setReply(replyText);
            setContext({
                intent: data.intent || data.action || null,
                location: contextData.location,
                actions: data.actions || [],
                ...data.context,
            });

            // TTS ì‘ë‹µ
            await synthTTSMultilingual(replyText);
        } catch (error) {
            console.error('ìŒì„± ì²˜ë¦¬ ì˜¤ë¥˜:', error);
            setReply('ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.');
        } finally {
            setListening(false);
        }
    }

    async function getCurrentLocation(): Promise<{ lat: number; lng: number } | null> {
        return new Promise((resolve) => {
            if (!navigator.geolocation) {
                resolve(null);
                return;
            }

            navigator.geolocation.getCurrentPosition(
                (position) => {
                    resolve({
                        lat: position.coords.latitude,
                        lng: position.coords.longitude,
                    });
                },
                () => {
                    resolve(null);
                }
            );
        });
    }

    return (
        <div className="p-4 space-y-4">
            <h2 className="text-xl font-bold">ğŸ¤ Voice UX 2.0 Assistant</h2>

            <div className="flex flex-col items-center gap-4">
                {/* ë¹„ë””ì˜¤ í”„ë¦¬ë·° */}
                <Card className="w-full max-w-md">
                    <CardContent className="p-4">
                        <video
                            ref={videoRef}
                            autoPlay
                            playsInline
                            muted
                            className="w-full h-48 rounded-xl bg-muted object-cover"
                        />
                        {gesture && (
                            <div className="mt-2 flex items-center gap-2">
                                <Hand className="w-4 h-4" />
                                <Badge variant="secondary">ì œìŠ¤ì²˜: {gesture}</Badge>
                            </div>
                        )}
                    </CardContent>
                </Card>

                {/* ìŒì„± ì…ë ¥ ë²„íŠ¼ */}
                <Button
                    size="lg"
                    onClick={handleVoice}
                    disabled={listening}
                    className="w-full max-w-md"
                >
                    {listening ? (
                        <>
                            <Mic className="w-5 h-5 mr-2 animate-pulse" />
                            ë“£ëŠ” ì¤‘...
                        </>
                    ) : (
                        <>
                            <Mic className="w-5 h-5 mr-2" />
                            ë§í•˜ê¸°
                        </>
                    )}
                </Button>

                {/* ì…ë ¥ í…ìŠ¤íŠ¸ */}
                {text && (
                    <Card className="w-full max-w-md">
                        <CardContent className="p-4">
                            <div className="text-sm font-semibold mb-2">ì…ë ¥:</div>
                            <div className="text-muted-foreground">{text}</div>
                        </CardContent>
                    </Card>
                )}

                {/* ì‘ë‹µ */}
                {reply && (
                    <Card className="w-full max-w-md">
                        <CardContent className="p-4 space-y-3">
                            <div className="flex items-center justify-between">
                                <div className="text-sm font-semibold">ì‘ë‹µ:</div>
                                <Button
                                    size="sm"
                                    variant="outline"
                                    onClick={() => synthTTSMultilingual(reply)}
                                >
                                    <Volume2 className="w-4 h-4 mr-1" />
                                    ì¬ìƒ
                                </Button>
                            </div>
                            <div className="text-muted-foreground">{reply}</div>

                            {/* ì»¨í…ìŠ¤íŠ¸ ì •ë³´ */}
                            {context && (
                                <div className="pt-2 border-t space-y-1 text-xs text-muted-foreground">
                                    {context.intent && (
                                        <div className="flex items-center gap-2">
                                            <Badge variant="outline">ì˜ë„: {context.intent}</Badge>
                                        </div>
                                    )}
                                    {context.location && (
                                        <div className="flex items-center gap-2">
                                            <MapPin className="w-3 h-3" />
                                            ìœ„ì¹˜: {context.location.lat?.toFixed(4)}, {context.location.lng?.toFixed(4)}
                                        </div>
                                    )}
                                    {context.actions && context.actions.length > 0 && (
                                        <div className="flex items-center gap-2">
                                            <Calendar className="w-3 h-3" />
                                            ì•¡ì…˜: {context.actions.length}ê°œ
                                        </div>
                                    )}
                                </div>
                            )}
                        </CardContent>
                    </Card>
                )}
            </div>
        </div>
    );
}

